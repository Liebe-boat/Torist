import streamlit as st
import pandas as pd
import os
import re

# ==========================================
# 0. å¤šè¯­è¨€é…ç½® & æ™ºèƒ½åˆ—æ’åºç­–ç•¥
# ==========================================
st.set_page_config(page_title="Torist Bird Index", layout="wide", page_icon="ğŸ¦")

# ç•Œé¢ç¿»è¯‘åŒ…
TRANSLATIONS = {
    "CN": {
        "title": "Torist ğŸ¦ æ™ºèƒ½é¸Ÿåæ£€ç´¢ç³»ç»Ÿ",
        "settings": "ç³»ç»Ÿè®¾ç½®",
        "data_status": "å·²åŠ è½½ç‰ˆæœ¬",
        "base_list": "é€‰æ‹©åŸºå‡†åå½• (Base)",
        "cross_ref": "æ·»åŠ å¯¹æ¯”ç‰ˆæœ¬ (Compare)",
        "search_label": "å…¨åº“æ£€ç´¢",
        "search_placeholder": "è¾“å…¥é¸Ÿå...",
        "found_res": "å…±æ‰¾åˆ° {count} ä¸ªåŒ¹é…",
        "col_view": "ğŸ“‹ {name} è¯¦æƒ…è§†å›¾",
        "synonym_loaded": "ğŸ”— åŒä¹‰è¯åº“: {count} æ¡è§„åˆ™",
        "no_data": "âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆåå½•ï¼Œè¯·æ£€æŸ¥ original_index æ–‡ä»¶å¤¹",
        "folder_missing": "âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨"
    },
    "EN": {
        "title": "Torist ğŸ¦ Smart Bird Index",
        "settings": "System Settings",
        "data_status": "Loaded Versions",
        "base_list": "Base Checklist",
        "cross_ref": "Compare With",
        "search_label": "Global Search",
        "search_placeholder": "Type bird name...",
        "found_res": "Found {count} matches",
        "col_view": "ğŸ“‹ View: {name}",
        "synonym_loaded": "ğŸ”— Synonyms: {count} rules",
        "no_data": "âš ï¸ No valid checklists found.",
        "folder_missing": "âŒ Folder missing"
    },
    "JP": {
        "title": "Torist ğŸ¦ é‡é³¥åæ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ",
        "settings": "è¨­å®š",
        "data_status": "èª­è¾¼æ¸ˆã¿ãƒªã‚¹ãƒˆ",
        "base_list": "åŸºæº–ãƒªã‚¹ãƒˆ (Base)",
        "cross_ref": "æ¯”è¼ƒãƒªã‚¹ãƒˆ (Compare)",
        "search_label": "æ¤œç´¢",
        "search_placeholder": "é³¥ã®åå‰ã‚’å…¥åŠ›...",
        "found_res": "{count} ä»¶ãƒ’ãƒƒãƒˆ",
        "col_view": "ğŸ“‹ {name} ãƒ“ãƒ¥ãƒ¼",
        "synonym_loaded": "ğŸ”— ã‚·ãƒãƒ‹ãƒ : {count} ä»¶",
        "no_data": "âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "folder_missing": "âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    }
}

# æ ¸å¿ƒåŠŸèƒ½ï¼šæ ¹æ®å½“å‰è¯­è¨€ï¼Œå†³å®šè¡¨æ ¼åˆ—çš„æ˜¾ç¤ºé¡ºåº
def get_column_priority(lang_code):
    """
    è¿”å›åˆ—åçš„ä¼˜å…ˆé¡ºåºåˆ—è¡¨
    """
    base_cols = ['å­¦å', 'Link_Key']
    if lang_code == 'CN':
        return base_cols + ['ä¸­æ–‡å', 'ä¸­æ–‡å_TW', 'English', 'Japanese', 'ç§‘å']
    elif lang_code == 'JP':
        return base_cols + ['Japanese', 'å’Œå', 'English', 'ä¸­æ–‡å', 'ä¸­æ–‡å_TW', 'Family']
    else: # EN
        return base_cols + ['English', 'English_IOC', 'Chinese', 'Japanese', 'Family']

# ==========================================
# 1. æ™ºèƒ½æ–‡ä»¶è¯»å– (æ”¯æŒå¤šç‰ˆæœ¬)
# ==========================================
def read_excel_smart(filepath, sheet_keywords, header_hints):
    """é€šç”¨è¯»å–å™¨ï¼Œè‡ªåŠ¨é€‚é… xls/xlsx å’Œè¡¨å¤´"""
    try:
        engine = 'openpyxl' if filepath.endswith('.xlsx') else 'xlrd'
        xls = pd.ExcelFile(filepath, engine=engine)
        
        # 1. æ‰¾ Sheet
        target_sheet = xls.sheet_names[0]
        if sheet_keywords:
            for name in xls.sheet_names:
                if any(k in name for k in sheet_keywords):
                    target_sheet = name
                    break
        
        # 2. æ‰¾è¡¨å¤´è¡Œ
        df_temp = pd.read_excel(xls, sheet_name=target_sheet, header=None, nrows=20)
        header_row = 0
        found_header = False
        
        for idx, row in df_temp.iterrows():
            row_str = row.astype(str).str.cat(sep=' ')
            # åªè¦åŒ…å«ä»»æ„ä¸€ä¸ªæç¤ºè¯ï¼Œå°±è®¤ä¸ºæ˜¯è¡¨å¤´
            if any(h in row_str for h in header_hints):
                header_row = idx
                found_header = True
                break
        
        # CBR ç‰¹æ®Šå¤„ç†
        if not found_header and "CBR" in filepath: header_row = 7

        df = pd.read_excel(xls, sheet_name=target_sheet, header=header_row)
        return df
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

def extract_version(filename):
    """ä»æ–‡ä»¶åæå–ç‰ˆæœ¬å· (ä¾‹å¦‚ v10.0, 2023, 15.1)"""
    # åŒ¹é… v10.0, 14.1, 2023 è¿™ç§æ ¼å¼
    match = re.search(r'(v\d+\.\d+|\d+\.\d+|20\d{2})', filename)
    if match:
        return match.group(1)
    return "Unknown"

# ==========================================
# 2. æ•°æ®åŠ è½½æ ¸å¿ƒ (åŠ¨æ€åŠ è½½æ‰€æœ‰æ–‡ä»¶)
# ==========================================
@st.cache_data
def load_data():
    data_store = {}
    base_dir = "original_index"
    syn_dict = {}

    if not os.path.exists(base_dir):
        return {}, {}, "Folder Missing"

    files = [f for f in os.listdir(base_dir) if f.endswith(('.xlsx', '.xls'))]
    
    # ---------------------------
    # A. å¾ªç¯åŠ è½½æ‰€æœ‰åå½•
    # ---------------------------
    for f in files:
        f_path = os.path.join(base_dir, f)
        version = extract_version(f)
        
        # 1. è¯†åˆ«ï¼šä¸­å›½åå½• (CBR)
        if "China" in f or "CBR" in f:
            df = read_excel_smart(f_path, ["Checklist", "æ­£è¡¨"], ['å­¦å', 'Scientific'])
            if df is not None:
                df.columns = df.columns.str.strip() # å»ç©ºæ ¼
                if 'å­¦å' in df.columns:
                    df = df[['å­¦å', 'ä¸­æ–‡å', 'è‹±æ–‡å']].dropna(subset=['å­¦å'])
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    # å…³é”®ï¼šç”Ÿæˆå”¯ä¸€çš„ Keyï¼Œä¾‹å¦‚ "China (v10.0)"
                    key = f"China ({version})"
                    data_store[key] = df

        # 2. è¯†åˆ«ï¼šå°æ¹¾åå½• (TW)
        elif "TW" in f:
            # é¡ºä¾¿åŠ è½½åŒä¹‰è¯ (å¦‚æœè¿™ä¸ªæ–‡ä»¶é‡Œæœ‰å˜åŠ¨è¡¨)
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç®€åŒ–é€»è¾‘ï¼Œæ¯æ¬¡é‡åˆ°TWæ–‡ä»¶éƒ½è¯•ç€è¯»ä¸€ä¸‹å˜åŠ¨è¡¨
            try:
                syn_df = read_excel_smart(f_path, ["è®Šå‹•", "Change"], ['è®Šå‹•ç´°é …'])
                if syn_df is not None:
                    col = next((c for c in syn_df.columns if 'è®Šå‹•ç´°é …' in c), None)
                    if col:
                        for txt in syn_df[col].dropna():
                            m = re.search(r'å­¸åï¼š([A-Za-z ]+)â†’([A-Za-z ]+)', str(txt))
                            if m:
                                syn_dict[m.group(1).strip()] = m.group(2).strip()
                                syn_dict[m.group(2).strip()] = m.group(1).strip()
            except: pass

            # åŠ è½½æ­£è¡¨
            df = read_excel_smart(f_path, ["æ­£è¡¨", "List"], ['å­¸å', 'Scientific'])
            if df is not None:
                df = df.rename(columns={'å­¸å': 'å­¦å', 'ä¸­æ–‡å': 'ä¸­æ–‡å_TW'})
                if 'å­¦å' in df.columns:
                    df = df[['å­¦å', 'ä¸­æ–‡å_TW', 'è‹±æ–‡å']].dropna(subset=['å­¦å'])
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    key = f"Taiwan ({version})"
                    data_store[key] = df

        # 3. è¯†åˆ«ï¼šæ—¥æœ¬åå½• (OSJ)
        elif "jp" in f.lower() or "osj" in f.lower():
            df = read_excel_smart(f_path, ["ãƒªã‚¹ãƒˆ", "List"], ['å­¦å', 'Scientific'])
            if df is not None:
                if 'ã‚«ãƒ†ã‚´ãƒª' in df.columns: df = df[df['ã‚«ãƒ†ã‚´ãƒª'] == 'ç¨®']
                if 'å­¦å' in df.columns and 'å’Œå' in df.columns:
                    df = df[['å­¦å', 'å’Œå']].dropna(subset=['å­¦å'])
                    df = df.rename(columns={'å’Œå': 'Japanese'})
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    key = f"Japan ({version})"
                    data_store[key] = df

        # 4. è¯†åˆ«ï¼šIOC (æ”¯æŒå¤šç‰ˆæœ¬!)
        elif "IOC" in f:
            df = read_excel_smart(f_path, ["List"], ['IOC', 'Scientific'])
            if df is not None:
                # åŠ¨æ€æ‰¾å­¦ååˆ— (IOC_14.1, IOC_15.1...)
                ioc_sci_col = next((c for c in df.columns if 'IOC' in c and 'Order' not in c), None)
                if ioc_sci_col:
                    cols_map = {ioc_sci_col: 'å­¦å', 'English': 'English_IOC'}
                    keep_cols = [ioc_sci_col, 'English', 'Chinese', 'Japanese', 'Family']
                    keep_cols = [c for c in keep_cols if c in df.columns] # åªä¿ç•™å­˜åœ¨çš„
                    
                    df = df[keep_cols].rename(columns=cols_map)
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    
                    # è¿™é‡Œçš„ Version ä¼šè‡ªåŠ¨å˜ï¼Œæ¯”å¦‚ "IOC (14.1)" å’Œ "IOC (15.1)"
                    key = f"IOC ({version})"
                    data_store[key] = df

    return data_store, syn_dict, "Success"

# ==========================================
# 3. ç•Œé¢é€»è¾‘ (UI)
# ==========================================

# A. è¯­è¨€åˆ‡æ¢
with st.sidebar:
    lang_opt = st.radio("Language / è¨€èª", ["ä¸­æ–‡", "English", "æ—¥æœ¬èª"], horizontal=True)
    lang_code = "CN" if lang_opt == "ä¸­æ–‡" else ("EN" if lang_opt == "English" else "JP")
    txt = TRANSLATIONS[lang_code]

st.title(txt["title"])

# B. æ•°æ®è¯»å–
data_dict, synonym_map, status = load_data()

if status == "Folder Missing":
    st.error(txt["folder_missing"])
    st.stop()
if not data_dict:
    st.error(txt["no_data"])
    st.stop()
if synonym_map:
    st.toast(txt["synonym_loaded"].format(count=len(synonym_map)), icon="ğŸ”—")

# C. ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.markdown("---")
    st.header(txt["settings"])
    
    # æ˜¾ç¤ºå·²åŠ è½½çš„æ‰€æœ‰ç‰ˆæœ¬
    with st.expander(txt["data_status"]):
        # æ’åºæ˜¾ç¤ºï¼Œå¥½çœ‹ä¸€ç‚¹
        for k in sorted(data_dict.keys()):
            st.success(f"âœ… {k}")

    # é€‰æ‹©åŸºå‡† (Base)
    # è¿™é‡Œçš„ keys å·²ç»æ˜¯åŠ¨æ€çš„äº†ï¼Œæ¯”å¦‚ "IOC (14.1)", "IOC (15.1)"
    base_list = st.selectbox(txt["base_list"], sorted(data_dict.keys()))
    
    # é€‰æ‹©å¯¹æ¯” (Compare)
    avail_opts = sorted([k for k in data_dict.keys() if k != base_list])
    # æ™ºèƒ½é»˜è®¤å€¼ï¼šå¦‚æœæ²¡é€‰ IOCï¼Œé»˜è®¤å‹¾é€‰ä¸€ä¸ª IOC çš„æœ€æ–°ç‰ˆ
    default_vals = []
    ioc_versions = [x for x in avail_opts if "IOC" in x]
    if ioc_versions:
        default_vals = [ioc_versions[-1]] # é€‰æœ€æ–°çš„ IOC
        
    compare_lists = st.multiselect(txt["cross_ref"], avail_opts, default=default_vals)

# ==========================================
# 4. æ ¸å¿ƒåˆå¹¶ä¸æ˜¾ç¤º (Display)
# ==========================================
main_df = data_dict[base_list].copy()

# å¾ªç¯åˆå¹¶
for target_name in compare_lists:
    target_df = data_dict[target_name].copy()
    target_sci_names = set(target_df['å­¦å'].values)
    
    # åŒä¹‰è¯é“¾æ¥
    def find_link_key(name):
        if name in target_sci_names: return name
        if name in synonym_map:
            alias = synonym_map[name]
            if alias in target_sci_names: return alias
        return None

    main_df['Link_Key'] = main_df['å­¦å'].apply(find_link_key)
    
    # ç®€åŒ–çš„åç¼€ï¼Œå»æ‰æ‹¬å·é‡Œçš„ redundant ä¿¡æ¯
    # ä¾‹å¦‚ "IOC (15.1)" -> "_IOC_15.1"
    suffix = "_" + target_name.replace(" ", "").replace("(", "").replace(")", "")
    
    merged = pd.merge(
        main_df,
        target_df,
        left_on='Link_Key',
        right_on='å­¦å',
        how='left',
        suffixes=('', suffix)
    )
    
    # æ¸…ç†
    if 'Link_Key' in merged.columns: del merged['Link_Key']
    if 'å­¦å' + suffix in merged.columns: del merged['å­¦å' + suffix]
        
    main_df = merged

# ç•Œé¢æ˜¾ç¤º
st.subheader(txt["col_view"].format(name=base_list))

col1, col2 = st.columns([3, 1])
with col1:
    query = st.text_input(txt["search_label"], placeholder=txt["search_placeholder"])

# --- æ™ºèƒ½åˆ—æ’åºé€»è¾‘ ---
# 1. è·å–æ‰€æœ‰å­˜åœ¨çš„åˆ—
all_cols = list(main_df.columns)
# 2. è·å–å½“å‰è¯­è¨€æ¨èçš„ä¼˜å…ˆåˆ— (æ¯”å¦‚æ—¥è¯­ä¼˜å…ˆæ˜¾ç¤º 'Japanese')
priority_cols = get_column_priority(lang_code)
# 3. æ’åºï¼šå…ˆæ”¾ä¼˜å…ˆåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå†æ”¾å‰©ä¸‹çš„åˆ—
final_col_order = []
for c in priority_cols:
    # æ¨¡ç³ŠåŒ¹é…åˆ—å (æ¯”å¦‚ 'Japanese' å¯ä»¥åŒ¹é… 'Japanese_IOC15.1')
    matched = [existing for existing in all_cols if c in existing or (c == 'ä¸­æ–‡å' and 'ä¸­æ–‡' in existing)]
    for m in matched:
        if m not in final_col_order:
            final_col_order.append(m)

# æŠŠå‰©ä¸‹çš„åˆ—è¡¥åœ¨åé¢
for c in all_cols:
    if c not in final_col_order:
        final_col_order.append(c)

# é‡æ–°æ’åºåˆ—
main_df = main_df[final_col_order]
# --------------------

if query:
    mask = main_df.astype(str).apply(lambda x: x.str.lower().str.contains(query.lower())).any(axis=1)
    res = main_df[mask]
    st.info(txt["found_res"].format(count=len(res)))
    st.dataframe(res, use_container_width=True, hide_index=True)
else:
    st.dataframe(main_df.head(200), use_container_width=True, hide_index=True)