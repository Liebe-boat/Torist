import streamlit as st
import pandas as pd
import os
import re

# ==========================================
# 0. å¤šèªè¨€é…ç½® & åˆ—åç¿»è­¯å­—å…¸ (å«ç°¡ç¹å€åˆ†)
# ==========================================
st.set_page_config(page_title="Torist Bird Index", layout="wide", page_icon="ğŸ¦")

# ==========================================
# 0. å¤šèªè¨€é…ç½® & åˆ—åç¿»è­¯å­—å…¸ (å·²æ›´æ–°ï¼šå€åˆ†ç°¡ç¹)
# ==========================================
st.set_page_config(page_title="Torist Bird Index", layout="wide", page_icon="ğŸ¦")

TRANSLATIONS = {
    "SC": { # ç°¡é«”ä¸­æ–‡
        "title": "Torist ğŸ¦ å¤šè¯­è¨€é¸Ÿç±»ç´¢å¼•",
        "settings": "ç³»ç»Ÿè®¾ç½®",
        "data_status": "å·²åŠ è½½ç‰ˆæœ¬",
        "base_list": "é€‰æ‹©åŸºå‡†åå½• (Base)",
        "cross_ref": "æ·»åŠ å¯¹æ¯”åå½• (Compare)",
        "search_label": "å…¨åº“æ£€ç´¢",
        "search_placeholder": "è¾“å…¥é¸Ÿå / ç¼–å·...",
        "found_res": "å…±æ‰¾åˆ° {count} ä¸ªåŒ¹é…",
        "col_view": "ğŸ“‹ {name}",
        "synonym_loaded": "ğŸ”— åŒä¹‰è¯åº“: {count} æ¡è§„åˆ™",
        "no_data": "âš ï¸ æœªæ£€æµ‹åˆ°æœ‰æ•ˆåå½•ï¼Œè¯·æ£€æŸ¥ original_index æ–‡ä»¶å¤¹",
        "folder_missing": "âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨"
    },
    "TC": { # ç¹é«”ä¸­æ–‡
        "title": "Torist ğŸ¦ å¤šèªè¨€é³¥é¡ç´¢å¼•",
        "settings": "ç³»çµ±è¨­ç½®",
        "data_status": "å·²åŠ è¼‰ç‰ˆæœ¬",
        "base_list": "é¸æ“‡åŸºæº–åéŒ„ (Base)",
        "cross_ref": "æ·»åŠ å°æ¯”åéŒ„ (Compare)",
        "search_label": "å…¨åº«æª¢ç´¢",
        "search_placeholder": "è¼¸å…¥é³¥å / ç·¨è™Ÿ...",
        "found_res": "å…±æ‰¾åˆ° {count} å€‹åŒ¹é…",
        "col_view": "ğŸ“‹ {name}",
        "synonym_loaded": "ğŸ”— åŒç¾©è©åº«: {count} æ¢è¦å‰‡",
        "no_data": "âš ï¸ æœªæª¢æ¸¬åˆ°æœ‰æ•ˆåéŒ„ï¼Œè«‹æª¢æŸ¥ original_index æ–‡ä»¶å¤¾",
        "folder_missing": "âŒ æ–‡ä»¶å¤¾ä¸å­˜åœ¨"
    },
    "EN": {
        "title": "Torist ğŸ¦ Smart Wild Bird Index",
        "settings": "System Settings",
        "data_status": "Loaded Versions",
        "base_list": "Base Checklist",
        "cross_ref": "Compare With",
        "search_label": "Global Search",
        "search_placeholder": "Type bird name / index...",
        "found_res": "Found {count} matches",
        "col_view": "ğŸ“‹ View: {name}",
        "synonym_loaded": "ğŸ”— Synonyms: {count} rules",
        "no_data": "âš ï¸ No valid checklists found.",
        "folder_missing": "âŒ Folder missing"
    },
    "JP": {
        "title": "Torist ğŸ¦ å¤šèªè¨€é‡é³¥åéŒ„",
        "settings": "è¨­å®š",
        "data_status": "èª­è¾¼æ¸ˆã¿ãƒªã‚¹ãƒˆ",
        "base_list": "åŸºæº–ãƒªã‚¹ãƒˆ (Base)",
        "cross_ref": "æ¯”è¼ƒãƒªã‚¹ãƒˆ (Compare)",
        "search_label": "æ¤œç´¢",
        "search_placeholder": "é³¥ã®åå‰ / ç•ªå·ã‚’å…¥åŠ›...",
        "found_res": "{count} ä»¶ãƒ’ãƒƒãƒˆ",
        "col_view": "ğŸ“‹ {name} ãƒ“ãƒ¥ãƒ¼",
        "synonym_loaded": "ğŸ”— ã‚·ãƒãƒ‹ãƒ : {count} ä»¶",
        "no_data": "âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "folder_missing": "âŒ ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
    }
}

# æ ¸å¿ƒï¼šåˆ—åç¿»è­¯ (æ–°å¢ï¼šä¸åŒè¦–è§’ä¸‹çš„åç¨±æ˜ å°„)
COLUMN_MAP = {
    "SC": { # ç°¡é«”è¦–è§’
        "Index": "ç¼–å·", "å­¦å": "å­¦å", "Scientific": "å­¦å",
        "ä¸­æ–‡å": "ä¸­æ–‡å", "Chinese": "ä¸­æ–‡å", # è‡ªå·±çš„èªè¨€
        "ä¸­æ–‡å_TW": "ä¸­æ–‡å(å°)", "Chinese (Traditional)": "ä¸­æ–‡å(ç¹)", # åˆ¥äººçš„èªè¨€
        "English": "è‹±æ–‡å", "English_IOC": "è‹±æ–‡å(IOC)",
        "Japanese": "æ—¥æ–‡å", "å’Œå": "æ—¥æ–‡å",
        "Family": "ç§‘å", "ç§‘å": "ç§‘å"
    },
    "TC": { # ç¹é«”è¦–è§’
        "Index": "ç·¨è™Ÿ", "å­¦å": "å­¸å", "Scientific": "å­¸å",
        "ä¸­æ–‡å_TW": "ä¸­æ–‡å", "Chinese (Traditional)": "ä¸­æ–‡å", # è‡ªå·±çš„èªè¨€
        "ä¸­æ–‡å": "ä¸­æ–‡å(ç°¡)", "Chinese": "ä¸­æ–‡å(ç°¡)", # åˆ¥äººçš„èªè¨€
        "English": "è‹±æ–‡å", "English_IOC": "è‹±æ–‡å(IOC)",
        "Japanese": "æ—¥æ–‡å", "å’Œå": "æ—¥æ–‡å",
        "Family": "ç§‘å", "ç§‘å": "ç§‘å"
    },
    "EN": {
        "Index": "#", "å­¦å": "Sci-Name", 
        "ä¸­æ–‡å": "Chinese(S)", "ä¸­æ–‡å_TW": "Chinese(T)",
        "Chinese": "Chinese(S)", "Chinese (Traditional)": "Chinese(T)",
        "English": "English", "English_IOC": "English(IOC)",
        "Japanese": "Japanese", "å’Œå": "Japanese",
        "Family": "Family", "ç§‘å": "Family"
    },
    "JP": {
        "Index": "No.", "å­¦å": "å­¦å", 
        "ä¸­æ–‡å": "ä¸­å›½èª(ç°¡)", "ä¸­æ–‡å_TW": "ä¸­å›½èª(ç¹)",
        "Chinese": "ä¸­å›½èª(ç°¡)", "Chinese (Traditional)": "ä¸­å›½èª(ç¹)",
        "English": "è‹±èª", "English_IOC": "è‹±èª(IOC)",
        "Japanese": "å’Œå", "å’Œå": "å’Œå",
        "Family": "ç§‘", "ç§‘å": "ç§‘"
    }
}

# åˆ—æ’åºå„ªå…ˆç´š (æ–°å¢ï¼šç°¡ç¹å„ªå…ˆé †åºä¸åŒ)
def get_column_priority(lang_code):
    base = ['Index', 'å­¦å', 'Link_Key']
    if lang_code == 'SC':
        # ç°¡é«”å„ªå…ˆ
        return base + ['ä¸­æ–‡å', 'Chinese', 'ä¸­æ–‡å_TW', 'Chinese (Traditional)', 'English', 'Japanese', 'Family', 'ç§‘å']
    elif lang_code == 'TC':
        # ç¹é«”å„ªå…ˆ
        return base + ['ä¸­æ–‡å_TW', 'Chinese (Traditional)', 'ä¸­æ–‡å', 'Chinese', 'English', 'Japanese', 'Family', 'ç§‘å']
    elif lang_code == 'JP':
        return base + ['Japanese', 'å’Œå', 'English', 'ä¸­æ–‡å', 'ä¸­æ–‡å_TW', 'Family', 'ç§‘å']
    else: # EN
        return base + ['English', 'English_IOC', 'Chinese (Traditional)', 'Chinese', 'Japanese', 'Family']

# æ ¸å¿ƒï¼šåˆ—åç¿»è­¯ (å€åˆ†ç°¡ç¹)
# é€™è£¡å®šç¾©äº† raw data çš„åˆ—å -> ç•Œé¢é¡¯ç¤ºçš„åˆ—å
COLUMN_MAP = {
    "SC": { # ç®€ä½“è§†è§’
        "Index": "ç¼–å·", "å­¦å": "å­¦å", "Scientific": "å­¦å",
        "ä¸­æ–‡å": "ä¸­æ–‡å", "Chinese": "ä¸­æ–‡å", # è‡ªå·±çš„è¯­è¨€
        "ä¸­æ–‡å_TW": "ä¸­æ–‡å(å°)", "Chinese (Traditional)": "ä¸­æ–‡å(ç¹)", # åˆ«äººçš„è¯­è¨€
        "English": "è‹±æ–‡å", "English_IOC": "è‹±æ–‡å(IOC)",
        "Japanese": "æ—¥æ–‡å", "å’Œå": "æ—¥æ–‡å",
        "Family": "ç§‘å", "ç§‘å": "ç§‘å"
    },
    "TC": { # ç¹ä½“è§†è§’
        "Index": "ç·¨è™Ÿ", "å­¦å": "å­¸å", "Scientific": "å­¸å",
        "ä¸­æ–‡å_TW": "ä¸­æ–‡å", "Chinese (Traditional)": "ä¸­æ–‡å", # è‡ªå·±çš„è¯­è¨€
        "ä¸­æ–‡å": "ä¸­æ–‡å(ç°¡)", "Chinese": "ä¸­æ–‡å(ç°¡)", # åˆ«äººçš„è¯­è¨€
        "English": "è‹±æ–‡å", "English_IOC": "è‹±æ–‡å(IOC)",
        "Japanese": "æ—¥æ–‡å", "å’Œå": "æ—¥æ–‡å",
        "Family": "ç§‘å", "ç§‘å": "ç§‘å"
    },
    "EN": {
        "Index": "#", "å­¦å": "Sci-Name", 
        "ä¸­æ–‡å": "Chinese(S)", "ä¸­æ–‡å_TW": "Chinese(T)",
        "Chinese": "Chinese(S)", "Chinese (Traditional)": "Chinese(T)",
        "English": "English", "English_IOC": "English(IOC)",
        "Japanese": "Japanese", "å’Œå": "Japanese",
        "Family": "Family", "ç§‘å": "Family"
    },
    "JP": {
        "Index": "No.", "å­¦å": "å­¦å", 
        "ä¸­æ–‡å": "ä¸­å›½èª(ç°¡)", "ä¸­æ–‡å_TW": "ä¸­å›½èª(ç¹)",
        "Chinese": "ä¸­å›½èª(ç°¡)", "Chinese (Traditional)": "ä¸­å›½èª(ç¹)",
        "English": "è‹±èª", "English_IOC": "è‹±èª(IOC)",
        "Japanese": "å’Œå", "å’Œå": "å’Œå",
        "Family": "ç§‘", "ç§‘å": "ç§‘"
    }
}

# åˆ—æ’åºå„ªå…ˆç´š (æ ¹æ“šèªè¨€ç¿’æ…£èª¿æ•´)
def get_column_priority(lang_code):
    base = ['Index', 'å­¦å', 'Link_Key']
    if lang_code == 'SC':
        # ç°¡é«”å„ªå…ˆï¼šSimple Chinese > Trad Chinese
        return base + ['ä¸­æ–‡å', 'Chinese', 'ä¸­æ–‡å_TW', 'Chinese (Traditional)', 'English', 'Japanese', 'Family', 'ç§‘å']
    elif lang_code == 'TC':
        # ç¹é«”å„ªå…ˆï¼šTrad Chinese > Simple Chinese
        return base + ['ä¸­æ–‡å_TW', 'Chinese (Traditional)', 'ä¸­æ–‡å', 'Chinese', 'English', 'Japanese', 'Family', 'ç§‘å']
    elif lang_code == 'JP':
        return base + ['Japanese', 'å’Œå', 'English', 'ä¸­æ–‡å', 'ä¸­æ–‡å_TW', 'Family', 'ç§‘å']
    else: # EN
        return base + ['English', 'English_IOC', 'Chinese (Traditional)', 'Chinese', 'Japanese', 'Family']

def translate_columns(df, lang_code):
    new_cols = {}
    map_dict = COLUMN_MAP.get(lang_code, {})
    for col in df.columns:
        base_part = col
        suffix_part = ""
        if "[" in col and col.endswith("]"):
            parts = col.split(" [")
            base_part = parts[0]
            suffix_part = " [" + parts[1]
        trans_base = map_dict.get(base_part, base_part)
        new_cols[col] = trans_base + suffix_part
    return df.rename(columns=new_cols)

# ==========================================
# 1. æ™ºèƒ½æ–‡ä»¶è®€å–èˆ‡æ¸…æ´—
# ==========================================
# 1. æ™ºèƒ½æ–‡ä»¶è®€å–èˆ‡æ¸…æ´—
# ==========================================
def read_excel_smart(filepath, sheet_keywords, header_hints):
    try:
        engine = 'openpyxl' if filepath.endswith('.xlsx') else 'xlrd'
        xls = pd.ExcelFile(filepath, engine=engine)
        target_sheet = xls.sheet_names[0]
        if sheet_keywords:
            for name in xls.sheet_names:
                if any(k in name for k in sheet_keywords):
                    target_sheet = name
                    break
        df_temp = pd.read_excel(xls, sheet_name=target_sheet, header=None, nrows=20)
        header_row = 0
        found_header = False
        for idx, row in df_temp.iterrows():
            row_str = row.astype(str).str.cat(sep=' ')
            if any(h in row_str for h in header_hints):
                header_row = idx
                found_header = True
                break
        if not found_header and "CBR" in filepath: header_row = 7
        df = pd.read_excel(xls, sheet_name=target_sheet, header=header_row)
        return df
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return None

# --- å‡ç´šå¾Œçš„ç‰ˆæœ¬æå–å‡½æ•¸ ---
def extract_version(filename):
    """
    æ™ºèƒ½æå–ç‰ˆæœ¬è™Ÿï¼Œæ”¯æŒå¤šç¨®æ ¼å¼
    """
    # 1. å„ªå…ˆåŒ¹é… "7ed", "8ed" é€™ç¨®æ˜ç¢ºçš„ç‰ˆæ¬¡ (æ—¥æœ¬åéŒ„å¸¸ç”¨)
    match = re.search(r'(\d+)ed', filename, re.IGNORECASE)
    if match:
        return f"{match.group(1)}th"

    # 2. åŒ¹é… "JP 7", "JP 8", "ver 7", "v7" é€™ç¨®æ ¼å¼
    # é‚è¼¯ï¼šJP/ver/v å¾Œé¢è·Ÿè‘—æ•¸å­—ï¼Œä¸”æ•¸å­—å¾Œé¢æ²’æœ‰å…¶ä»–æ•¸å­—äº†
    match = re.search(r'(?:JP|ver|v)[ ._-]?(\d+)(?!\d)', filename, re.IGNORECASE)
    if match:
        return f"v{match.group(1)}"

    # 3. åŸæœ‰çš„åŒ¹é…è¦å‰‡ (v10.0, 15.1, 2023)
    match = re.search(r'(v\d+\.\d+|\d+\.\d+|20\d{2})', filename)
    if match:
        return match.group(1)

    return "Unknown"

def clean_index(val):
    try:
        if pd.isna(val): return ""
        s = str(val).strip()
        if s.endswith(".0"): return s[:-2]
        return s
    except: return str(val)

# ==========================================
# 2. æ•¸æ“šåŠ è¼‰æ ¸å¿ƒ
# ==========================================
@st.cache_data
def load_data():
    data_store = {}
    base_dir = "original_index"
    syn_dict = {}

    if not os.path.exists(base_dir): return {}, {}, "Folder Missing"
    files = [f for f in os.listdir(base_dir) if f.endswith(('.xlsx', '.xls'))]
    
    for f in files:
        f_path = os.path.join(base_dir, f)
        version = extract_version(f)
        
        # 1. China (CBR) - ä¿®æ”¹é€™è£¡ï¼
        if "China" in f or "CBR" in f:
            df = read_excel_smart(f_path, ["Checklist", "æ­£è¡¨"], ['å­¦å', 'Scientific'])
            if df is not None:
                df.columns = df.columns.str.strip()
                if 'å­¦å' in df.columns:
                    if 'ç¼–å·' in df.columns: df = df.rename(columns={'ç¼–å·': 'Index'})
                    else: df['Index'] = range(1, len(df) + 1)
                    df = df[['Index', 'å­¦å', 'ä¸­æ–‡å', 'è‹±æ–‡å']].dropna(subset=['å­¦å'])
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    df['Index'] = df['Index'].apply(clean_index)
                    
                    # === æ”¹å‹•é»ï¼šé€™è£¡åŠ ä¸Šäº† CBR å­—æ¨£ ===
                    key = f"China CBR ({version})" 
                    data_store[key] = df

        # 2. Taiwan
        elif "TW" in f:
            try:
                syn_df = read_excel_smart(f_path, ["è®Šå‹•", "Change"], ['è®Šå‹•ç´°é …'])
                if syn_df is not None:
                    col = next((c for c in syn_df.columns if 'è®Šå‹•ç´°é …' in c), None)
                    if col:
                        for txt in syn_df[col].dropna():
                            m = re.search(r'å­¸åï¼š([A-Za-z ]+)â†’([A-Za-z ]+)', str(txt))
                            if m:
                                syn_dict[m.group(1).strip()] = m.group(2).strip()
                                syn_dict[m.group(2).strip()] = m.group(1).strip()
            except: pass

            df = read_excel_smart(f_path, ["æ­£è¡¨", "List"], ['å­¸å', 'Scientific'])
            if df is not None:
                rename_map = {'å­¸å': 'å­¦å', 'ä¸­æ–‡å': 'ä¸­æ–‡å_TW'}
                if 'ç·¨ç¢¼' in df.columns: rename_map['ç·¨ç¢¼'] = 'Index'
                elif 'Code' in df.columns: rename_map['Code'] = 'Index'
                
                df = df.rename(columns=rename_map)
                
                if 'å­¦å' in df.columns:
                    cols = ['å­¦å', 'ä¸­æ–‡å_TW', 'è‹±æ–‡å']
                    if 'Index' in df.columns: cols.insert(0, 'Index')
                    df = df[cols].dropna(subset=['å­¦å'])
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    if 'Index' in df.columns: df['Index'] = df['Index'].apply(clean_index)
                    data_store[f"Taiwan ({version})"] = df

        # 3. Japan (OSJ)
        elif "jp" in f.lower() or "osj" in f.lower():
            # === å°ˆé–€è™•ç†ç¬¬ 7 ç‰ˆ (èˆŠæ ¼å¼ .xls) ===
            # v7 ç‰¹å¾µï¼šæ–‡ä»¶åå« 7edï¼Œå…§å®¹ç„¡è¡¨é ­ï¼Œå­¸ååˆ†å…©åˆ—
            if "7ed" in f or "v7" in version:
                try:
                    # 1. å¼·åˆ¶ç”¨ xlrd è®€å–ï¼Œä¸”ä¸è¨­è¡¨é ­ (header=None)ï¼Œé€™æ¨£æˆ‘å€‘å¯ä»¥ç”¨æ•¸å­—ç´¢å¼•åˆ—
                    df = pd.read_excel(f_path, header=None, engine='xlrd')
                    
                    # 2. ç¯©é¸ï¼šç¬¬1åˆ—æ˜¯ç­‰ç´šï¼Œæˆ‘å€‘åªè¦ "ç¨®" (Species)
                    # v7 çµæ§‹é€šå¸¸æ˜¯: [No, Rank, ID, Genus, Species, Auth, JapName...]
                    # ç´¢å¼•:           0    1    2     3       4       5       6
                    if 1 in df.columns:
                        df = df[df[1] == 'ç¨®']
                    
                    # 3. åˆä½µå­¸å (å±¬å + ç©ºæ ¼ + ç¨®å°å)
                    if 3 in df.columns and 4 in df.columns:
                        df['å­¦å'] = df[3].astype(str) + " " + df[4].astype(str)
                        
                    # 4. æå–æ—¥æ–‡å (é€šå¸¸åœ¨ç¬¬ 6 åˆ—ï¼Œæœ‰æ™‚å€™åœ¨ç¬¬ 7 åˆ—ï¼Œä¿éšªèµ·è¦‹è©¦ä¸€ä¸‹)
                    if 6 in df.columns:
                        df['Japanese'] = df[6]
                    
                    # 5. æå–ç·¨è™Ÿ (ç¬¬ 0 åˆ—)
                    if 0 in df.columns:
                        df['Index'] = df[0]
                        
                    # 6. æ•´ç†ä¸¦å­˜å…¥
                    if 'å­¦å' in df.columns and 'Japanese' in df.columns:
                        df = df[['Index', 'å­¦å', 'Japanese']]
                        df['å­¦å'] = df['å­¦å'].str.strip()
                        if 'Index' in df.columns: df['Index'] = df['Index'].apply(clean_index)
                        
                        key = f"Japan ({version})"
                        data_store[key] = df
                        
                except Exception as e:
                    print(f"Error reading Japan v7: {e}")

            # === è™•ç†ç¬¬ 8 ç‰ˆ (v8) åŠæ¨™æº–æ ¼å¼ (ä¿æŒä¸è®Š) ===
            else:
                df = read_excel_smart(f_path, ["ãƒªã‚¹ãƒˆ", "List"], ['å­¦å', 'Scientific'])
                if df is not None:
                    if 'ã‚«ãƒ†ã‚´ãƒª' in df.columns: df = df[df['ã‚«ãƒ†ã‚´ãƒª'] == 'ç¨®']
                    
                    idx_col = None
                    for c in ['ç¨®ç•ªå·', 'æ²è¼‰é †', 'No', 'Seq']:
                        if c in df.columns:
                            idx_col = c
                            break
                    
                    rename_map = {'å’Œå': 'Japanese'}
                    if idx_col: rename_map[idx_col] = 'Index'
                    
                    df = df.rename(columns=rename_map)
                    
                    if 'å­¦å' in df.columns and 'Japanese' in df.columns:
                        cols = ['å­¦å', 'Japanese']
                        if 'Index' in df.columns: cols.insert(0, 'Index')
                        df = df[cols].dropna(subset=['å­¦å'])
                        df['å­¦å'] = df['å­¦å'].str.strip()
                        if 'Index' in df.columns: df['Index'] = df['Index'].apply(clean_index)
                        data_store[f"Japan ({version})"] = df

#       4. IOC
        elif "IOC" in f:
            df = read_excel_smart(f_path, ["List"], ['IOC', 'Scientific'])
            if df is not None:
                df.columns = [c.strip() for c in df.columns]
                idx_col = next((c for c in df.columns if c.lower() in ['seq', 'rank', 'no.']), None)
                ioc_sci_col = next((c for c in df.columns if 'IOC' in c and 'Order' not in c), None)
                
                if ioc_sci_col:
                    cols_map = {ioc_sci_col: 'å­¦å', 'English': 'English_IOC'}
                    if idx_col: cols_map[idx_col] = 'Index'
                    
                    df = df.rename(columns=cols_map)
                    
                    # === é€™è£¡å¢åŠ äº† Chinese (Traditional) ===
                    keep_cols = ['å­¦å', 'English_IOC', 'Chinese', 'Chinese (Traditional)', 'Japanese', 'Family']
                    
                    if 'Index' in df.columns: keep_cols.insert(0, 'Index')
                    keep_cols = [c for c in keep_cols if c in df.columns]
                    
                    df = df[keep_cols]
                    df['å­¦å'] = df['å­¦å'].str.strip()
                    if 'Index' in df.columns: df['Index'] = df['Index'].apply(clean_index)
                    data_store[f"IOC ({version})"] = df

    return data_store, syn_dict, "Success"

# ==========================================
# 3. ç•Œé¢é‚è¼¯
# ==========================================
with st.sidebar:
    # é€™è£¡å¢åŠ äº† "ç®€ä½“ä¸­æ–‡" å’Œ "ç¹é«”ä¸­æ–‡" çš„é¸é …
    lang_opt = st.radio("Language / è¨€èª", ["ç®€ä½“ä¸­æ–‡", "ç¹é«”ä¸­æ–‡", "English", "æ—¥æœ¬èª"], horizontal=True)
    
    # é‚è¼¯æ˜ å°„
    if lang_opt == "ç®€ä½“ä¸­æ–‡": lang_code = "SC"
    elif lang_opt == "ç¹é«”ä¸­æ–‡": lang_code = "TC"
    elif lang_opt == "English": lang_code = "EN"
    else: lang_code = "JP"
    
    txt = TRANSLATIONS[lang_code]

st.title(txt["title"])

data_dict, synonym_map, status = load_data()

if status == "Folder Missing":
    st.error(txt["folder_missing"])
    st.stop()
if not data_dict:
    st.error(txt["no_data"])
    st.stop()
if synonym_map:
    st.toast(txt["synonym_loaded"].format(count=len(synonym_map)), icon="ğŸ”—")

with st.sidebar:
    st.markdown("---")
    st.header(txt["settings"])
    with st.expander(txt["data_status"]):
        for k in sorted(data_dict.keys()):
            st.success(f"âœ… {k}")
            
    base_list = st.selectbox(txt["base_list"], sorted(data_dict.keys()))
    
    avail_opts = sorted([k for k in data_dict.keys() if k != base_list])
    default_vals = []
    ioc_versions = [x for x in avail_opts if "IOC" in x]
    if ioc_versions: default_vals = [ioc_versions[-1]]
    compare_lists = st.multiselect(txt["cross_ref"], avail_opts, default=default_vals)

# ==========================================
# 4. æ ¸å¿ƒåˆä½µ
# ==========================================
main_df = data_dict[base_list].copy()

for target_name in compare_lists:
    target_df = data_dict[target_name].copy()
    target_sci_names = set(target_df['å­¦å'].values)
    
    def find_link_key(name):
        if name in target_sci_names: return name
        if name in synonym_map:
            alias = synonym_map[name]
            if alias in target_sci_names: return alias
        return None

    main_df['Link_Key'] = main_df['å­¦å'].apply(find_link_key)
    
    rename_map = {}
    for col in target_df.columns:
        if col == 'å­¦å': continue
        rename_map[col] = f"{col} [{target_name}]"
    
    target_df_renamed = target_df.rename(columns=rename_map)
    cols_to_use = ['å­¦å'] + list(rename_map.values())
    
    merged = pd.merge(
        main_df,
        target_df_renamed[cols_to_use], 
        left_on='Link_Key',
        right_on='å­¦å',
        how='left'
    )
    
    if 'Link_Key' in merged.columns: del merged['Link_Key']
    if 'å­¦å_y' in merged.columns: del merged['å­¦å_y']
    if 'å­¦å_x' in merged.columns: merged = merged.rename(columns={'å­¦å_x': 'å­¦å'})
    main_df = merged

# ==========================================
# 5. é¡¯ç¤ºå„ªåŒ–
# ==========================================
st.subheader(txt["col_view"].format(name=base_list))
col1, col2 = st.columns([3, 1])
with col1:
    query = st.text_input(txt["search_label"], placeholder=txt["search_placeholder"])

# æ’åº
all_cols = list(main_df.columns)
priority_basic_cols = get_column_priority(lang_code)
final_col_order = []

for p_col in priority_basic_cols:
    if p_col in all_cols: final_col_order.append(p_col)

for p_col in priority_basic_cols:
    matches = [c for c in all_cols if c.startswith(p_col + " [")]
    for m in matches:
        if m not in final_col_order: final_col_order.append(m)

for c in all_cols:
    if c not in final_col_order: final_col_order.append(c)

main_df = main_df[final_col_order]
display_df = translate_columns(main_df, lang_code)

if query:
    mask = main_df.astype(str).apply(lambda x: x.str.lower().str.contains(query.lower())).any(axis=1)
    res = display_df[mask]
    st.info(txt["found_res"].format(count=len(res)))
    st.dataframe(res, use_container_width=True, hide_index=True)
else:
    st.dataframe(display_df.head(200), use_container_width=True, hide_index=True)